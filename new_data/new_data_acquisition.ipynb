{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from imdbpie import Imdb\n",
    "from time import sleep\n",
    "import time\n",
    "import requests\n",
    "import string\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_df(list_of_shows, year):\n",
    "    temp_df = pd.DataFrame(list_of_shows, columns=['id', 'title'])\n",
    "    df_name = \"shows_\" + str(year) + '.csv'\n",
    "    temp_df.to_csv(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes shows between 2000 and 2015\n",
    "## maybe do it by year\n",
    "## this doesn't work for some reason\n",
    "\n",
    "# all_shows = []\n",
    "# for page in range(1,111):\n",
    "#     url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=2000,2015&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "#     soup2000 = BeautifulSoup(url2000)\n",
    "#     print \"scraping page :\"\n",
    "#     print page,\n",
    "#     for table in soup2000('div', {'class': 'lister-list'}):\n",
    "#         for span in table('span', {'class': 'lister-item-header'}):\n",
    "#             for title in span('a'):\n",
    "#                 imdbid = str(title).split('/')[2]\n",
    "#                 name = str(title).split('>')[1].split('<')[0]\n",
    "#                 all_shows.append([imdbid,name])\n",
    "#                 sleep(1)\n",
    "#     sleep(1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Page: \n",
      "1 2 3 4 5 6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "# 2015 shows\n",
    "\n",
    "shows2015 = []\n",
    "print \"Scraped Page: \"\n",
    "for page in range(1,11):\n",
    "    url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=2015,2015&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "    soup2000 = BeautifulSoup(url2000)\n",
    "    for table in soup2000('div', {'class': 'lister-list'}):\n",
    "        for span in table('span', {'class': 'lister-item-header'}):\n",
    "            for title in span('a'):\n",
    "                imdbid = str(title).split('/')[2]\n",
    "                name = str(title).split('>')[1].split('<')[0]\n",
    "                shows2015.append([imdbid, name])\n",
    "                sleep(1)\n",
    "    sleep(2)\n",
    "    print page,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_to_df(shows2015, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Page: \n",
      "1 2 3 4 5 6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "## 2014 shows\n",
    "\n",
    "shows2014 = []\n",
    "print \"Scraped Page: \"\n",
    "for page in range(1,11):\n",
    "    url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=2014,2014&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "    soup2000 = BeautifulSoup(url2000)\n",
    "    for table in soup2000('div', {'class': 'lister-list'}):\n",
    "        for span in table('span', {'class': 'lister-item-header'}):\n",
    "            for title in span('a'):\n",
    "                imdbid = str(title).split('/')[2]\n",
    "                name = str(title).split('>')[1].split('<')[0]\n",
    "                shows2014.append([imdbid, name])\n",
    "                sleep(1)\n",
    "    sleep(5)\n",
    "    print page,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_to_df(shows2014, 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Page: \n",
      "1 2 3 4 5 6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "## 2013 shows\n",
    "\n",
    "shows2013 = []\n",
    "print \"Scraped Page: \"\n",
    "for page in range(1,11):\n",
    "    url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=2013,2013&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "    soup2000 = BeautifulSoup(url2000)\n",
    "    for table in soup2000('div', {'class': 'lister-list'}):\n",
    "        for span in table('span', {'class': 'lister-item-header'}):\n",
    "            for title in span('a'):\n",
    "                imdbid = str(title).split('/')[2]\n",
    "                name = str(title).split('>')[1].split('<')[0]\n",
    "                shows2013.append([imdbid, name])\n",
    "                sleep(1)\n",
    "    sleep(2)\n",
    "    print page,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_to_df(shows2013, 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shows2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Page: \n",
      "1 2 3 4 5 6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "## 2012 shows\n",
    "years = '2012,2012'\n",
    "shows2012 = []\n",
    "print \"Scraped Page: \"\n",
    "for page in range(1,11):\n",
    "    url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=' + years + '&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "    soup2000 = BeautifulSoup(url2000)\n",
    "    for table in soup2000('div', {'class': 'lister-list'}):\n",
    "        for span in table('span', {'class': 'lister-item-header'}):\n",
    "            for title in span('a'):\n",
    "                imdbid = str(title).split('/')[2]\n",
    "                name = str(title).split('>')[1].split('<')[0]\n",
    "                shows2013.append([imdbid, name])\n",
    "                sleep(1)\n",
    "    sleep(2)\n",
    "    print page,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_to_df(shows2012, 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Page: \n",
      "1 2 3 4 5 6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "## 2011 shows\n",
    "year = '2011'\n",
    "years = year + ',' + year\n",
    "shows2011 = []\n",
    "print \"Scraped Page: \"\n",
    "for page in range(1,11):\n",
    "    url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=' + years + '&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "    soup2000 = BeautifulSoup(url2000)\n",
    "    for table in soup2000('div', {'class': 'lister-list'}):\n",
    "        for span in table('span', {'class': 'lister-item-header'}):\n",
    "            for title in span('a'):\n",
    "                imdbid = str(title).split('/')[2]\n",
    "                name = str(title).split('>')[1].split('<')[0]\n",
    "                shows2013.append([imdbid, name])\n",
    "                sleep(1)\n",
    "    sleep(2)\n",
    "    print page,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_to_df(shows2011, 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shows2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## scrape all years\n",
    "\n",
    "## I made a better way of doing this\n",
    "\n",
    "# for i in range(1995,2017):\n",
    "#     year = str(i)\n",
    "#     years = year + ',' + year\n",
    "#     shows_temp = []\n",
    "#     print \"Scraping year: \", year\n",
    "#     print \"Scraped Page: \"\n",
    "#     for page in range(1,11):\n",
    "#         url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=' + years + '&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "#         soup2000 = BeautifulSoup(url2000)\n",
    "#         for table in soup2000('div', {'class': 'lister-list'}):\n",
    "#             for span in table('span', {'class': 'lister-item-header'}):\n",
    "#                 for title in span('a'):\n",
    "#                     imdbid = str(title).split('/')[2]\n",
    "#                     name = str(title).split('>')[1].split('<')[0]\n",
    "#                     shows_temp.append([imdbid, name])\n",
    "#                     sleep(2)\n",
    "#         sleep(5)\n",
    "#         print page,\n",
    "#     temp_df = pd.DataFrame(shows_temp, columns=['id', 'title'])\n",
    "#     df_name = \"shows_\" + year + '.csv'\n",
    "#     temp_df.to_csv(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function for scraping a single year\n",
    "## scrapes 2500 shows for the given year\n",
    "## exports as dataframe with name 'shows_[year].csv'\n",
    "\n",
    "def scrape_year(year):\n",
    "    year = str(year)\n",
    "    years = year + ',' + year\n",
    "    shows_temp = []\n",
    "    print \"Scraping Year: \", year\n",
    "    print \"Scraped Pages: \"\n",
    "    for page in range(1,11):\n",
    "        url2000 = urllib2.urlopen('http://www.imdb.com/search/title?count=250&countries=us&languages=en&production_status=released&release_date=' + years + '&title_type=tv_series&view=simple&sort=release_date,desc&page=' + str(page) + '&ref_=adv_nxt')\n",
    "        soup2000 = BeautifulSoup(url2000)\n",
    "        for table in soup2000('div', {'class': 'lister-list'}):\n",
    "            for span in table('span', {'class': 'lister-item-header'}):\n",
    "                for title in span('a'):\n",
    "                    imdbid = str(title).split('/')[2]\n",
    "                    name = str(title).split('>')[1].split('<')[0]\n",
    "                    shows_temp.append([imdbid, name])\n",
    "        print page,\n",
    "        sleep(5)\n",
    "    temp_df = pd.DataFrame(shows_temp, columns=['id', 'title'])\n",
    "    df_name = \"shows_\" + year + '.csv'\n",
    "    temp_df.to_csv(df_name)\n",
    "    print \"csv file created for year \", year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_year(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping year:  2016\n",
      "Scraped Page: \n",
      "1 2 3 4 5 6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "scrape_year(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Year:  2010\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "scrape_year(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Year:  2009\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2008\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2007\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2006\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2005\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2004\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2003\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2002\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2001\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  2000\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  1999\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  1998\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  1997\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  1996\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n",
      "Scraping Year:  1995\n",
      "Scraped Pages: \n",
      "1 2 3 4 5 6 7 8 9 10 csv file created\n"
     ]
    }
   ],
   "source": [
    "## I don't know for sure whether the earlier years have 2500 shows each\n",
    "## running them in reverse chronological order seems safest\n",
    "\n",
    "for i in range(2009, 1994, -1):\n",
    "    scrape_year(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shows = pd.DataFrame(shows2015, columns=['id', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt3524810</td>\n",
       "      <td>Point Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt4798620</td>\n",
       "      <td>The Pioneers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt4908938</td>\n",
       "      <td>Star Trek: Outlaws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt4655474</td>\n",
       "      <td>Joking Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt4708572</td>\n",
       "      <td>American Scandals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id               title\n",
       "0  tt3524810       Point Society\n",
       "1  tt4798620        The Pioneers\n",
       "2  tt4908938  Star Trek: Outlaws\n",
       "3  tt4655474          Joking Off\n",
       "4  tt4708572   American Scandals"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_shows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
